---
title: ACL2020 走馬觀花 - 1
categories:
 - Paper Reading
tags: 論文解析
---

前言
這個系列將會介紹最近研討會的成果，大量地介紹論文，看看他們在解決甚麼問題，有什麼新奇的方法，結果如何。
本篇會介紹以下的論文:  
A Unified MRC Framework for Named Entity Recognition   
DeFormer: Decomposing Pre-traicned Transformers for Faster Question Answering   
Enhancing Answer Boundary Detection for Multilingual Machine Reading Comprehension   
Improving Truthfulness of Headline Generation   
Asking and Answering Questions to Evaluate the Factual Consistency of Summaries   
FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization   
FastBERT: a Self-distilling BERT with Adaptive Inference Time   
DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference   
The Right Tool for the Job: Matching Model and Instance Complexities     


## A Unified MRC Framework for Named Entity Recognition
https://arxiv.org/abs/1910.11476


---

當NER遇到有nested的情況，如PEBP2 site會是DNA同時，而EPBP2也是protein，EPBP2就會有同屬兩個NER的情況。   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/pn_acl2020_1/R9uPeni-2.png)
傳統的sequence tagging方式只適合標一個詞性，因此這篇論文提出用QA的方式去做NER。   
QA的方式是指，給一篇文章，根據某個類型的Entity發問，然後用標記start-end的方式找出所有該類別的NER，如希望找出所有地點的Entity，就如下發問：   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/pn_acl2020_1/FZTekps-2.png)
所以我們問PROTEIN就可以得到EPBP2，問DNA就可以得到PEBP2 site，互不干擾。同時，問題也可以提供更多的訊息，使得在沒有看過的label上也有一定的處理能力。   
看點：
- 怎麼提問效果最好？
- QA只會標注一個start跟End，怎麼做到標記多個entity？  

思考：
- 需要每一個類型的NER都預測一次，效率會減慢
- 可否用 conditional tagging 來做？
- 用QA的dataset作為pretrain會不會有幫助呢？

## DeFormer: Decomposing Pre-traicned Transformers for Faster Question Answering  
https://arxiv.org/abs/2005.00697

---

提高QA的效率，關鍵是在於transformer需要經過多層的運算，導致效率拖慢。減少層數又會影響到準確度，因此這篇論文提出對QA的passage和question分解，找出獨立並可以預先預算出來的部分，從而減少預測時所要跑的層數   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/pn_acl2020_1/J8gSy2L.png)
Transformer雖然是全局運算的，但在不同層級所關注的部分也不同。一般認為較低層級的會關注局部的文本，到高層級才會關注在全部文本。前K層說不定就只會關注passage本身，question也是如此。因此前K層passage和question的encode就可以預先做啦。   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/pn_acl2020_1/EsrtsgY.png)
看點：
- 可以預先encode的K層，K是多少效果最好?
- 怎麼樣銜接Decompose的兩個向量
- 如何保持效果？
    - Knowledge Distillation
    - Layerwise Representation Similarity

結果：
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/pn_acl2020_1/yf4tUZ0.png)

思考：
- 用這個方法處理長度超過512的問題？ 

## Enhancing Answer Boundary Detection for Multilingual Machine Reading Comprehension
https://arxiv.org/abs/2004.14069

---

用多語言模型處理其他語言的資料時，QA的效果相比句子分類下降很多   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/pn_acl2020_1/xrlMZ0l.png)
其中原因可能是QA還需要學會不同語言的切法，來判斷正確的邊界。錯誤的邊界會直接影響到輸出的結果。因此，這篇論文提出兩個方向去解決這個問題：
- MixMRC   
將query或passage翻譯成其他語言，再混合訓練，來增強對多語言在QA上的表現：
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/pn_acl2020_1/yTSJHaA.png)
- Language-agnostic Knowledge Phrase Masking   
對於不同語言的資料做phrase masking，找一些相關領域，有意義的phrase去做masklm。
這個方法往往需要大量的資料才有效果，因此也提出一個從網路上找phrase跟passage的方法
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/pn_acl2020_1/gtOIZWJ.png)
結果：
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/pn_acl2020_1/jyUwnos.png)
可見這個方法在對於人名，動物名字這種專有名詞的答案上有不少的提升，對於數字類型的data提升則沒有那麼大，也是符合 要學會不同語言的切法 這個設想   

## Improving Truthfulness of Headline Generation
https://arxiv.org/abs/2005.00882
## Asking and Answering Questions to Evaluate the Factual Consistency of Summaries
https://arxiv.org/abs/2004.04228
## FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization
https://arxiv.org/abs/2005.03754

---

這三篇都在處理同一類的問題：用Token Score來驗證生成的結果並不可信，所以希望找出原因或提出新的驗證方法。   

Improving Truthfulness of Headline Generation發現生成出來的文本，出現不是事實的情況，是因為訓練資料有問題。因而提出用textual entailment來驗證生成的文本是否蘊含在原文裡。可惜人工部分有點多。   

Asking and Answering Questions to Evaluate the Factual Consistency of Summaries 和 FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization 則是希望用QA的方式去查驗生成的文本。   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/pn_acl2020_1/FLU8rVS.png)
在生成出來的summary上，mask一些字作為答案，生成對應的問題。然後將問題套到源文本上，看看回答跟生成的回答是否相似，越相似表示生成的summary越好。   

思考:
這幾篇論文都是屬於發現問題類型的，現在的文本生成雖然可以做到句子流暢，但邏輯是否一致，生成的文本是否正確，還沒有一個很好的方法去解決。   

## FastBERT: a Self-distilling BERT with Adaptive Inference Time
https://arxiv.org/abs/2004.02178
## DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference
https://arxiv.org/abs/2004.12993
## The Right Tool for the Job: Matching Model and Instance Complexities
https://arxiv.org/abs/2004.02178

---

又是想法一樣的三篇，嘗試去提高transformer的速度。之前也提到過，現在transformer的效率取決於模型的層數，模型層數越多，預測時間越長。因此，這幾篇論文都提出early exit的方法，每一層都跑一次預測，當模型預測的結果很確定，就直接輸出。使得在一些簡單的任務上，可以加快預測速度。在比較困難的任務上，也可以保持準確度。   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/pn_acl2020_1/aqPOK1t.png)
至於如何衡量模型是否確定答案，Entropy會是一個不錯的選擇。模型預測的機率分佈越接近uniform，表示模型傾向隨機預測，模型不確定答案為何，entropy會越大。    
所以當Entropy超過一定的值，可以評量為不確定的輸出，丟到下一層繼續預測。  
 
看點：      
- 如何選定threshold？
- 加distillation為什麼會有效？   


思考：   
- 模型的不確定真的是可信的嗎？可以參考out of distribution這一類問題的研究。 