---                            
title: 解析詞向量3 word2vec有趣的特性                            
categories:                            
 - WordEmb101                            
tags: 解析詞向量                            
---                            
                  
這篇文章講的是：          
- word2vec 向量大小的含義          
- word2vec 的向量方向          
- King - men + woman = Queen ?    
- fastText 介紹          
    
Word2Vec的模型大致就是這樣，丟到其他任務中看看效果怎麽樣之前，也想看看這個embedding到底發現了什麽訊息，一看嚇一跳，從中有著不少有趣的特性：    
向量包含 大小和方向 ，我們訓練出來的詞向量也是如此，先從這裏入手    
詞向量的大小，也就是向量的Magnitude，它的大小其實與詞頻有關    
Measuring Word Significance using Distributed Representations of Words - 這一篇中對其有詳細探討    
他們發現，經常出現在不同内容的詞，也是極高頻率的詞 - 的，嗎，我    
他們的向量會因爲不同的内容而被拉扯到各個方向，最終的長度就像是不同方向的平均，導致其向量長度變小。    
而那些頻率相對低一些，在特定内容很長提及的詞，他們都往同一個方向更新，向量的長度就可以更加長了    
而頻率很低的詞，由於更新次數不多，所以長度也不會太長    
綜合來看，就得到了這張圖：    
    
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/wordemb/img16)    
    
    
而從方向來看，我們發現，有著相似上下文的詞，他們都會往著某個方向更新，所以方向上相近的詞，他們的上下文也很相似    
這種相似是可替換性的相似，比如    
「我們 去 六福村 玩 雲霄飛車 好不好」    
「我們 去 麗寶樂園 玩 雲霄飛車 好不好」    
「我們 去 九族文化村 玩 雲霄飛車 好不好」    
有點同義詞的意味，但又不太一樣，比如    
「我們 喜歡 你」    
「我們 討厭 你」    
喜歡跟討厭 的上下文都一樣，因此他們相似度很接近，但意思卻是相反了。    
在分佈式假説下，同義詞跟反義詞都會混在一起，模型也就沒有能力去區分兩者的差別    
所以，有時候看到Google翻譯會出錯，也可能是因爲他們的神經網絡用了 分佈式假説的embedding 作爲輸入導致的    
    
![https://news.now.com/home/technology/player?newsId=301441](https://images-news.now.com/newsimage/NewsImage/2018-04-06-16-28-29wgNPZPBp.jpg)    
    
    
King - men + woman = Queen ?    
類比英文是Analogies，指的是 A is to x as B is to y 的結構    
一個有名的例子是    
man is to kingking as woman is to queen    
如果    
man is to kingking as woman is to ?    
這個問題剛好可以用Word2Vec解決，得出這樣的等式：    
$$\mathbf{w}_{king} - \mathbf{w}_{man} + \mathbf{w}_{woman} \approx \mathbf{w}_{queen}$$    
這一個關係在Word2Vec的設計中，是并沒有考慮到的，更加像是一個意外得到的產物。    
圖形化表示，會像這樣的    
    
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/wordemb/img17)    
    
    
實際上當然沒有那麽完美，類比並不適用於所有的例子，而$$\mathbf{w}_{king} - \mathbf{w}_{man} + \mathbf{w}_{woman}$$也不會完全等於Queen    
其中必然存在限制，導致類比的效果沒有我們想象的那麽好    
在Analogies Explained: Towards Understanding Word Embeddings這一篇論文以及作者的blog也對此有詳細探討，簡單地總結一下，是有三個因素：    
Paraphrase error(ρ)：由於詞與詞之間的上下文總不會完全相等(不然就是同一個詞了XD)，因此相似的詞也總是相近，而不會重合。還有多少距離重合，會是影響因素之一    
Conditional independence Error(σ)：要衡量類比關係的詞，有越相近的上下文越好，比如man，king，woman，queen上下文都有一定相似    
Independence Error(τ)：這些類比的pair應該有著一定的獨立性，比如 king,man 與 woman,queen    
這三個因素也是造成爲什麽$$\mathbf{w}_{king} - \mathbf{w}_{man} + \mathbf{w}_{woman}$$不會完全等於Queen的原因    
    
![https://carl-allen.github.io/assets/solution.png](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/wordemb/img18)    
    
    
也是因爲這些誤差的存在，嚴格限制類比可以發生的情況，也使得類別沒有在所有情況下都是準確。    
    
fastText    
這個其實也是word2vec作者 - Tomas Mikolov 參與的研究    
裏面介紹一個借鑒word2vec的分類器/一個是word2vec的小改進    
一個詞也可以拆成字為單位 - 比如 指鹿爲馬 可拆成 指 鹿 爲 馬    
如果經過訓練，讓 守株待兔/指/鹿/爲/馬 的詞向量都很接近    
當有人打錯字，變成 指驢爲馬，除了說查無此字之外，還可以拿出指/為/馬的詞向量組合起來，得到一個接近 守株待兔 的詞向量，就可以讓Word2vec解決未出現詞的問題啦    
    
可是有個問題是：指鹿爲馬拆出來的馬，跟猴年馬月的馬是不一樣的，也更單獨 馬 字有差異    
我們應該想辦法區分開。而處理方法很取巧：我們在一個詞的前後加上<和>，來表示開頭的結尾    
<指鹿爲馬> ，拆成字來看則是：<指/鹿/爲/馬>。而<猴年馬月>則會是：<猴/年/馬/月>，單獨的一個馬則是：<馬>。我們得到 馬> / 馬 / <馬> 三種不同的馬。這樣就能區分是拆出來的字還是單獨的詞。    
    
訓練的時候使得 詞 和 子詞 skip-gram/cbow的結果都一樣，最終的詞向量也是 - 詞本身和子字的和    
    
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/wordemb/img19    
    
    
    
參考資料    
http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf    
https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf        
https://www.aclweb.org/anthology/W15-1513        
https://github.com/tmikolov/word2vec       
https://arxiv.org/pdf/1508.02297.pdf    
https://arxiv.org/pdf/1905.09866.pdf    
https://arxiv.org/pdf/1810.04882.pdf    
https://carl-allen.github.io/nlp/2019/07/01/explaining-analogies-explained.html       