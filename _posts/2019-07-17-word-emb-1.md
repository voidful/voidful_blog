---                  
title: 解析詞向量1 word2vec是怎麼一步步想出來的                  
categories:                  
 - WordEmb101                  
tags: 解析詞向量                  
---                  
        
技術都不是突然之間出現的        
在外面熟悉的word2vec誕生之前，到底發生了什麽？        
是什麽引導到word2vec的提出呢？        
在這裏，我們一步步看下這段歷史的過程，代入設計者的思維，對於之後的理解相信會有不少幫助~        

    
詞向量的發展過程        
要用電腦去處理文本，首先是要讓電腦可以理解文本。        
因此需要將文本換成數字來表示        
最簡單方法是one-hot encoding        
給每個詞一個id，向量長度就是詞表的長度        
        
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/wordemb/img1)         
        
        
這個方法簡單，效率高        
要表示詞組，可以用ngram        
        
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/wordemb/img2)        
        
        
要表示句子，則是bag-of-words        
        
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/wordemb/img3)        
        
        
one hot/bag of words 這種方式建立表示這個詞在不在文本中這樣簡單的關係        
讓電腦理解文本，只有這些訊息是遠遠不夠的        
因此，除了將文本換成數字表示外，還希望這些數字本身能包含語義訊息 - 如重要性/詞語含義/上下文關係等等        
        
一個做法是給與不同的詞語不同的權重，使重要的字權重更高        
什麽是重要的詞呢，這裏介紹兩個有名的想法        
        
一個是考慮詞出現的頻率，越常出現的詞，重要度應該越高，但太長出現的往往是沒有什麽意義的詞 - 如的/嗎/他/我 之類的停用詞。        
很常出現，而且是某個領域的專有名詞，才是我們認爲重要的詞，因此多加一個考量因素 - 這個詞應該在某些文本中有出現，而不是所有的文本，越少文本有出現這個詞，重要性越高。        
這個想法得出的指標就是 - TF/IDF        
        
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/wordemb/img4)        
        
        
        
另外一個想法取自Google 的 PageRank，PageRank 的想法是一個重要的網頁之所以重要，是因爲有其他重要的網頁指向它。換到詞來看，一個詞之所以重要，是因爲有其他重要的詞在旁邊。這個方法叫做TextRank        
        
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/wordemb/img5)        
        
        
這個做法也只是給詞語一個權重，也沒能反映出他們之間的關係，我們需要另闢蹊徑去建立描述詞與詞之間關係的數學模型        
        
統計上的模型這時候可以大派用場了        
有個想法就是 - 事物之間的關聯度不一樣，那麽 他們相遇的機率 跟 他們剛好碰到一起的機率 也會不一樣        
若 相遇的機率 比 剛好碰到一起的機率 要高，表明他們關聯度比較高        
若 剛好碰到一起的機率 比 相遇的機率 要高，表明他們關聯度很低        
換成數學會是        
    $$    \frac{P(ab)}{P(a)*P(b)}  $$          
這個指標也就是        
Mutual Information        
        
詞語之間算出Mutual Information，還是上面weight的方法，這些訊息會以矩陣的形式存放，隨著詞表擴張，這個矩陣也變得十分龐大，因此還衍生出一系列矩陣化簡的方法        
SVD \ pLSA \ LSI \ PCA \ PLSA          
        
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/wordemb/img6)        
        
        
還剩下一點點要補充一下的：        
將一些詞變成數字，通常是向量，這個詞的數字化表示就是詞向量 word vector。        
將詞放到向量空間中的方法，叫做word embedding        
        
經過這些做法以後，成功做到將詞語變成數字，也有著以下缺點：        
        
- 新的詞進來，改變詞表大小，矩陣也隨之改變，需要重新化簡        
- 詞與詞之間可能並沒有一起出現過，會存在0的結果        
- 很難建立與多個詞直接的關係，運算量增長嚴重        
- 有詞關聯的訊息，卻沒有詞相似的訊息        
        
這時，另外一派，帶著語言模型走了出來        
語言模型是在文本層面建立的模型，衡量某個文本出現的機率        
比如這個文本出現的機率 - 今天天氣不錯        
設   $$  w_{1}  $$   為 今天        
  $$  w_{2}  $$   為 天氣        
  $$  w_{3}  $$   為 不錯        
可以表示成   $$  P(w_{1}w_{2}w_{3})  $$          
後文是根據前文而來，因此可以變成成        
  $$  P(w_{1}w_{2}w_{3})=P(w_{1})P(w_{2}\mid w_{1})P(w_{3}\mid w_{1}w_{2})  $$          
在文本很長的情況下，算出這一串東西還是很麻煩        
再引入一個假設，文本的下一個字的產生與前文N個字有關，而不是所有。引入這個假設可以有效減低運算量，而這個假設，就是馬克科夫假設(Markov property)        
然後這個看前面n個詞的模型也叫做ngram model        
$$  P(w_{1},\ldots ,w_{m})=\prod _{i=1}^{m}P(w_{i}\mid w_{1},\ldots ,w_{i-1})\approx \prod _{i=1}^{m}P(w_{i}\mid w_{i-(n-1)},\ldots ,w_{i-1}) $$         
        
如果將語言模型像之前一樣用矩陣化簡的方式建立模型，其實也會遇到同樣的問題:        
        
- 新的詞進來，改變詞表大小，矩陣也隨之改變，需要重新化簡        
- 詞與詞之間可能並沒有一起出現過，會存在0的結果        
- 很難建立與多個詞直接的關係，運算量增長嚴重        
- 有詞關聯的訊息，卻沒有詞相似的訊息        
        
為了更好地學到文本語意，Bengio在2003年發表的《A Neural Probabilistic Language Model》提出NNLM這一模型，也就是用神經網路建立語言模型        
NNLM的架構是        
        
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/wordemb/img7)        
        
        
NNLM第一步將前文N個詞，通過Matrix C，得到m大小的詞向量。        
將前文的N個詞向量接起來，得到一個n*m大小的向量        
將這個向量丟到一個feed-forward network softmax出下一個詞的機率        
其中，有一個值得注意的亮點 - Matrix C        
在論文中稱爲word features layer，由於Matrix C是share的，就是説所有input的詞都經過同一個matrix C，經過網絡更新之後，這個MatrixC蘊含了一些word的feature！        
有人會好奇，這個feature到底學到了什麽？挖下去之後，就有了的Word2Vec的誕生。        
Word2Vec也算是語言模型的副產物XD        
        
距離NNLM到Word2Vec，我們還差一小步        
NNLM在當時有幾個難以解決的缺點 -         
        
- 在那個時代，RNNLM的運算量十分驚人，在 14 million (13,994,528) 詞的語料中，跑5個epoch用了40個CPU 三個禮拜        
- NNLM只看前面的N個詞，沒能很好捉到上下文的訊息        
        
這些問題，在運算量爆炸的今天都是輕鬆的        
想要更好獲取上下文訊息，可以用transformer嘛，效果不好，資料堆上去，用上TUP，也就有了新時代的狂暴型 NNLP - elmo/bert/gpt        
        
在運算量極大提高，能用狂暴方法解決這兩個問題之前。        
我們順著解決這些問題的思路，看看由此發掘出來的有趣產物吧        
        
        
 參考資料：        
 https://en.wikipedia.org/wiki/Language_model        
 http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf        