---                                     
title: Bertä¹‹å¾Œçš„é€²å±•?ç¸½çµå„ç¨®æ¨¡å‹çš„æƒ³æ³•èˆ‡æ–¹å‘                                     
categories:                                     
 - Implement                                     
tags: å¯¦ä½œ                                     
---                                     
   
Bertå‡ºä¾†éƒ½æœ‰ä¸€æ®µä¸çŸ­çš„æ™‚é–“äº†ï¼Œé€™æ®µæ™‚é–“NLPç•Œç‹‚æš´å¼åœ°ç™¼å±•ï¼Œå„ç¨®æ¨¡å‹å’Œæ–¹æ³•è®“äººç›®ä¸æš‡æ¥ã€‚åœ¨é€™è£¡æ•´ç†äº†è¿‘æœŸçš„é€²å±•å’Œç›¸é—œçš„è³‡æºï¼Œä¹Ÿå¯ä»¥è—‰æ­¤çŒœæƒ³å¾€å¾Œçš„æ–¹å‘ã€‚   
   
å¦‚æœæ˜¯å°Bertä¸å¤ªç­è§£çš„è©±ï¼Œå¯ä»¥å…ˆåƒè€ƒçœ‹çœ‹ä¹‹å‰çš„æ–‡ç« ï¼š   
https://voidful.github.io/voidful_blog/implement/2019/06/05/bert-implement-101/      
   
æ¥ä¸‹ä¾†æœƒå¾ä»¥ä¸‹æ–¹å‘å»æ¢è¨:   
   
- é è¨“ç·´çš„æ–¹å¼   
    - MaskedLM æ”¹å–„ - åŠ å¤§ç¯„åœ   
    - æ”¹è®ŠMaskedçš„æ¯”ä¾‹   
    - NextSentencePrediction ğŸ‘?   
    - å…¶ä»–pre-train task   
- è¼•é‡åŒ–   
    - å‰ªæ   
    - çŸ©é™£åˆ†è§£   
    - åƒæ•¸å…±äº«   
- å¤šèªè¨€   
- æ›´å¤§çš„æ¨¡å‹ï¼Œæ›´å¥½çš„çµæœ?   
- å¤šä»»å‹™   
- ç›¸é—œä¸­æ–‡project   

   
## é è¨“ç·´çš„æ–¹å¼   
   
Berté€™å€‹æ¨¡å‹ç‚ºä»€éº¼æœƒå¼•èµ·é‚£éº¼å¤§çš„è®Šé©ï¼Œæ˜¯ç”±æ–¼ä»–æ”¹è®ŠNLP DLæ¨¡å‹çš„è¨“ç·´æ–¹å¼ã€‚å…ˆæ˜¯ç”¨å¤§è¦æ¨¡çš„èªæ–™è¨“ç·´ä¸€å€‹å­¸ç¿’èªæ„çš„æ¨¡å‹ï¼Œå†ç”¨é€™å€‹æ¨¡å‹å»åšç‰¹åœ°çš„ä»»å‹™ - é–±è®€ç†è§£/æƒ…ç·’åˆ†é¡/NERâ€¦..   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img1.png)   
   
   
é€™ç¨®æ–¹å¼ä¹Ÿè¢«Yann LeCunå«åšself-supervises learning    
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img2.png)   
   
   
åœ¨Bertä¸Šï¼Œå­¸ç¿’èªæ„çš„æ¨¡å‹æ˜¯ä¸€å€‹åŸºæ–¼Transformer Encoderçš„å¤šä»»å‹™æ¨¡å‹ï¼Œåšçš„å…©å€‹ä»»å‹™åˆ†åˆ¥æ˜¯ MaskedLM å’Œ NextSentencePredictionã€‚   
   
è¦æ”¹å–„Bertï¼Œå…¶ä¸­ä¸€å€‹ç›´è¦ºçš„æƒ³æ³•ï¼Œå°±æ˜¯å¾é è¨“ç·´çš„ä»»å‹™å…¥æ‰‹ï¼š   
**MaskedLM æ”¹å–„ - åŠ å¤§ç¯„åœ**   
MaskedLMä¸­ï¼Œæœƒåªå°ä¸€å€‹å­—åšMASKï¼Œç”±æ–¼å­—èˆ‡å­—å’Œè©èªé–“çš„é—œè¯æ€§æ˜¯ä¸ä¸€æ¨£çš„ï¼ŒBertå¯èƒ½æ²’è¾¦æ³•å­¸åˆ°è©çµ„è·Ÿå­—çš„è¯ç¹«ã€‚å¦‚è™è ä¸€èµ·å‡ºç¾çš„æ©Ÿç‡å¾ˆé«˜ï¼Œæˆ‘å€‘å–®ç¨é æ¸¬ è™ æˆ–è€… è  çš„æ„ç¾©ä¸å¤§ï¼Œåè€Œæ˜¯é æ¸¬æ•´ä¸² è™è  æ›´åŠ èƒ½å­¸åˆ°å…¶èªç¾©ã€‚   
å› æ­¤ï¼Œæ“´å¤§maskingçš„ç¯„åœå°±å‹¢åœ¨å¿…è¡Œï¼Œè€Œä¸”ç¯„åœé‚„ä¸€ç´šä¸€ç´šå¾€ä¸Šå¢é•·ï¼š   
WordPieceæœƒæŠŠä¸€å€‹è‹±æ–‡å–®è©æ–·é–‹ï¼Œå› æ­¤è¦åœ¨å®Œæ•´å–®è©ä¸ŠMaskingçš„ - Google wwm   
æœ‰åœ¨Word/Phraseå±¤ç´šä¸Šmasking - ç™¾åº¦çš„ERNIE   
æ“´å¤§åˆ°ä¸€å®šé•·åº¦çš„ç‰‡æ®µ - Googleçš„Ngram Masking å’Œ Span Masking   
Word/Phraseå±¤ç´šéœ€è¦æä¾›ç›¸å°æ‡‰çš„è©è¡¨ï¼Œé€™äº›äººå·¥åŠ å…¥çš„è¨Šæ¯å¯ä»¥æœƒæ“¾äº‚åˆ°æ¨¡å‹ï¼Œæˆ–è€…çµ¦æ¨¡å‹ä¸€å€‹biasã€‚é€™å€‹ç¼ºé»çœ‹ä¾†ï¼Œæ“´å¤§åˆ°ä¸€å®šé•·åº¦çš„ç‰‡æ®µæ‡‰æ˜¯ä¸€å€‹æ¯”è¼ƒå¥½çš„è§£æ±ºæ–¹æ³•ï¼Œä½†T5å°ä¸åŒé•·åº¦çš„ç‰‡æ®µåšmaskingå¾—å‡ºé€™æ¨£çš„çµè«–ï¼š   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img3.png)   
   
   
å¯ä»¥çœ‹å‡ºï¼ŒåŠ å¤§é•·åº¦èªªæœ‰æ•ˆæœçš„ï¼Œä½†ä¸¦ä¸ä»£è¡¨è¶Šé•·è¶Šå¥½ã€‚SpanBerté€éæ©Ÿç‡å–æ¨£ï¼Œæ¸›å°‘Maskéé•·æ–‡æœ¬çš„æ©Ÿç‡æ˜¯ä¸éŒ¯çš„è§£æ±ºæ–¹æ³•ã€‚   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img4.png)   
   
   
SpanBertçš„å¯¦é©—çµæœï¼š   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img5.png)   
   
   
åªå¯æƒœç›®å‰ç›¸é—œçš„æ¨¡å‹é‚„å°‘ï¼Œå¯ä»¥å…ˆå–ç”¨word maskingçš„æ¨¡å‹æ”¹å–„æ•ˆæœï¼š   
è‹±æ–‡ç‰ˆå¯ä»¥å¾HuggingFaceçš„pre-trained modelæ‰¾whole-word-maskingçš„æ¨¡å‹   
https://huggingface.co/transformers/pretrained_models.html   
ä¸­æ–‡çš„è©±ï¼Œå¯ä»¥ç”¨ymcuiæ”¾å‡ºçš„pre-trained model   
https://github.com/ymcui/Chinese-BERT-wwm#   
 
   
   
**æ”¹è®ŠMaskedçš„æ¯”ä¾‹**   
Googleçš„T5å˜—è©¦ä¸åŒmaskedçš„æ¯”ä¾‹ï¼Œæ¢è¨æœ€å¥½çš„åƒæ•¸è¨­å®šæ˜¯ä»€éº¼ã€‚å¾ˆå‰›å¥½çš„ï¼ŒBertåŸå§‹è¨­å®šçš„15%å°±æ˜¯æœ€ä½³çš„é¸æ“‡ï¼š   
   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img6.png)   
   
   
**NextSentencePrediction ğŸ‘ï¼Ÿ**   
NSPé€éé æ¸¬å…©å€‹å¥å­æ˜¯å¦ç‚ºä¸Šä¸‹æ–‡é—œä¿‚ï¼Œå»å­¸å¥å­å±¤ç´šçš„è¨Šæ¯ã€‚åœ¨Bertçš„è«–æ–‡ä¸­ï¼Œå¯ä»¥ç™¼ç¾ä»–çš„æ•ˆæœä¸æ˜¯å¾ˆæ˜é¡¯ï¼Œç”šè‡³åœ¨æœ‰ä¸€äº›taskä¸Šé¢é‚„ä¸‹é™äº†ã€‚   
   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img7.png)   
   
   
NSPå¥½åƒæ•ˆæœä¸å¤ªå¥½å•Šï¼é€™é¦¬ä¸Šæˆäº†å¤§å®¶ç¾¤èµ·åœæ”»çš„åœ°æ–¹ï¼Œå…¶å¾Œçš„è«–æ–‡XLNET/RoBERTa/ALBERTä¹Ÿéƒ½è¸©å®ƒä¸€è…³ XD    
RoBERTa   
   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img8.png)   
   
   
ALBERT   
   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img8.png)   
   
   
XLNet   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img10.png)   
   
   
å¾€å¾Œçš„è«–æ–‡ä¹Ÿéƒ½ç™¼ç¾ï¼ŒNSPå¸¶ä¾†æ•ˆæœæ›´å¤šæ˜¯è² é¢çš„ï¼é€™å¯èƒ½æ˜¯ç”±æ–¼NSPé€™å€‹ä»»å‹™æœ¬èº«è¨­è¨ˆä¸åˆç†å°è‡´ - ä½œç‚ºè² æ¨£æœ¬çš„ä¸‹æ–‡æ˜¯å¾åˆ¥çš„æ–‡æª”æŠ½æ¨£è€Œä¾†ï¼Œå¯èƒ½å¤ªå®¹æ˜“åˆ†è¾¨ï¼Œå°è‡´å¾ä¸­å­¸åˆ°çš„æ±è¥¿ä¸å¤šï¼Œåè€Œæœƒè¢«é€™äº›è² æ¨£æœ¬å¹²æ“¾åˆ°; å†è€…ï¼ŒNSPè®“è¼¸å…¥è®Šæˆå…©å€‹ä¸ä¸€æ¨£çš„å¥å­ï¼Œç¼ºä¹é•·å¥å­æ¨£æœ¬ï¼Œä½¿å¾—Bertæ‡‰ä»˜é•·å¥å­çš„è¼¸å…¥æ•ˆæœä¸ä½³ã€‚   
   
ç”¨Bertä¾†åšåˆ†é¡ä»»å‹™çš„æ¨¡å‹ï¼Œå¦‚ï¼š**BertForSequenceClassification / BertForMultipleChoice**    
å°±æ˜¯åŸºæ–¼NSPè€Œä¾†çš„ã€‚ç”±æ–¼NSPçš„æ•ˆæœä¸¦ä¸å‡ºçœ¾ï¼Œä¹Ÿæ˜¯å»ºè­°æ”¹ç”¨å…¶ä»–é è¨“ç·´æ¨¡å‹ï¼Œä»¥å–å¾—æ›´å¥½çš„æ•ˆæœã€‚   
   
**å…¶ä»–pre-train task**   
æ—¢ç„¶NSPæ•ˆæœå¹³å¹³ï¼Œæœƒä¸æœƒæœ‰æ›´å¥½çš„é è¨“ç·´æ–¹å¼å‘¢ï¼Ÿå¤§å®¶å˜—è©¦äº†å„å¼å„æ¨£çš„æ–¹æ³•ï¼Œæˆ‘è¦ºå¾—ç›®å‰æœ€èƒ½ç¸½çµå‡ºå„ç¨®pretrain taskæ˜¯Googleçš„T5 å’Œ FBçš„BART   
T5æ‰€å˜—è©¦éçš„æ–¹å¼   
   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img11.png)   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img12.png)   
   
   
BARTæ‰€å˜—è©¦çš„æ–¹å¼   
   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img13.png)   
   
   
é€šå¸¸language modelæœƒä½œç‚ºå¤§å®¶çš„baselineï¼Œä¹‹å¾Œå˜—è©¦æœ‰   
   
- é®ä½ ä¸€äº›Tokenï¼Œé æ¸¬é®ä½çš„æ˜¯ä»€éº¼   
- æ‰“äº‚å¥å­é †åºï¼Œé æ¸¬æ­£ç¢ºé †åº   
- åˆªæ‰ä¸€äº›tokenï¼Œé æ¸¬å“ªè£¡è¢«åˆªæ‰   
- éš¨æ©ŸæŒ‘tokenï¼Œåœ¨æ­¤ä»¥å¾Œçš„å…§å®¹éƒ½æ¬åˆ°é–‹é ­ï¼Œé æ¸¬å“ªè£¡æ‰æ˜¯æ­£ç¢ºçš„é–‹é ­   
- åŠ å…¥ä¸€äº›tokenï¼Œé æ¸¬å“ªè£¡è¦åˆªæ‰   
- æ›¿æ›ä¸€äº›tokenï¼Œé æ¸¬å“ªè£¡è¢«æ›¿æ›äº†   
   
å¯¦é©—çš„çµæœå¦‚ä¸‹ï¼š   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img14.png) 
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img15.png)   
   
   
å¯¦é©—çµæœç™¼ç¾ï¼Œå¥½åƒæœ€åŸå§‹çš„MaskedLMå°±æ˜¯æœ€å¥½çš„çµæœäº†ï¼Œè¦æ•ˆæœæ›´å¥½ï¼Œç”¨ä¹‹å‰æåˆ°Span Maskingï¼Œç‚ºäº†é¿å…æ´©éœ²å‡ºå¤šå°‘å­—è¢«Maskedçš„è¨Šæ¯ï¼Œå¯ä»¥åªæ¨™è¨˜ä¸€å€‹maskï¼Œé æ¸¬å‡ºä¸€å€‹æˆ–è€…å¤šå€‹å­—çš„çµæœ   
   
   
## è¼•é‡åŒ–   
   
Bertçš„æ¨¡å‹å¾ˆå¤§ï¼Œç‚ºäº†è®“é‹è¡Œæ™‚çš„é€Ÿåº¦æ›´åŠ å¿«ï¼Œå¦ä¸€å€‹æ–¹å‘å°±æ˜¯è¼•é‡åŒ–æ¨¡å‹ã€‚   
All The Ways You Can Compress BERTå°æ­¤åšäº†è©³ç›¡çš„æ•´ç†ã€‚   
å…¶ä¸­çš„æ–¹å‘æœ‰ï¼š   
   
- å‰ªæ - åˆªæ‰æ¨¡å‹çš„ä¸€äº›éƒ¨åˆ†ï¼Œåˆªæ‰æŸäº›å±¤ï¼ŒæŸäº›head   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img16.png)   
   
- çŸ©é™£åˆ†è§£ - å°æ–¼è©è¡¨/åƒæ•¸åšçŸ©é™£åˆ†è§£   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img17.png)   
   
- çŸ¥è­˜è’¸é¤¾ - æŠŠBertçš„â€å­¸è­˜â€æ”¾åˆ°å…¶ä»–å°æ¨¡å‹ä¸Š   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img18.png)   
   
- åƒæ•¸å…±äº« - å±¤èˆ‡å±¤ä¹‹é–“å…±ç”¨åŒä¸€å€‹weight   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img19.png)   
   
   
æ¨¡å‹åŠæ•ˆæœå¯ä»¥åƒè€ƒåŸæ–‡   
http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html   
   
   
## å¤šèªè¨€   
   
NLPç•Œçš„æ•¸æ“šé›†å¾ˆä¸å¹³è¡¡ï¼Œé€šå¸¸æ˜¯æœ‰è‘—å¤§é‡è‹±æ–‡çš„è³‡æ–™é›†ï¼Œå…¶ä»–èªè¨€è³‡æ–™ç›¸å°è¼ƒå°‘ã€‚åœ¨ç¹é«”ä¸­æ–‡ä¸Šï¼Œé€™å€‹å•é¡Œæ›´åŠ åš´é‡ã€‚ç”±æ–¼bertçš„é è¨“ç·´æ–¹å¼ä¸¦æ²’æœ‰èªè¨€ä¸Šçš„é™åˆ¶ï¼Œå› è€Œå…¶ä¸­ä¸€å€‹æ–¹å‘ï¼Œæ˜¯å˜—è©¦å°‡æ›´å¤šèªè¨€çš„è³‡è¨Šæ”¾å…¥pre-trainæ¨¡å‹ä¸­ï¼Œå¸Œæœ›åœ¨ä¸‹æ¸¸ä»»å‹™ä¸Šï¼Œå–å¾—æ›´å¥½çš„æ•ˆæœã€‚   
   
ç›´æ¥å¯ç”¨çš„å°±æ˜¯Googleæ”¾å‡ºä¾†çš„Bert-Multilingualï¼Œå®ƒåœ¨æ²’æœ‰ä»»ä½•è³‡æ–™çš„ä¸‹æ¸¸ä»»å‹™ä¸Šï¼Œå–å¾—çµæœååˆ†æ¥è¿‘ä¸­æ–‡æ¨¡å‹   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img20.png)   
   
   
åœ¨Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Modelæ›´æ˜¯ç™¼ç¾ï¼Œå¤šèªè¨€ç‰ˆæœ¬çš„Bertåœ¨SQuAD(è‹±æ–‡é–±è®€ç†è§£ä»»å‹™)ä¸Šè¨“ç·´ï¼Œæ‹¿åˆ°DRCD(ä¸­æ–‡é–±è®€ç†è§£ä»»å‹™)é æ¸¬ï¼Œå°±å¯ä»¥é”åˆ°èˆ‡QANetæ¥è¿‘çš„çµæœï¼›è€Œä¸”å¤šèªè¨€çš„æ¨¡å‹ï¼Œä¸å°‡è³‡æ–™ç¿»è­¯ç‚ºåŒä¸€èªè¨€çš„çµæœï¼Œæœƒæ¯”ç¿»è­¯è¦å¥½ã€‚   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img21.png)   
   
   
ä»¥ä¸Šçš„çµæœï¼Œéƒ½èªªæ˜Bertå…§å­¸æœƒå°‡ä¸åŒèªè¨€çš„è³‡æ–™é€£çµèµ·ä¾†ï¼Œå¯èƒ½åœ¨Embeddingï¼Œä¹Ÿå¯èƒ½åœ¨transformer encoderä¸Š   
Emerging Cross-lingual Structure in Pretrained Language Modelså°±å¸Œæœ›ç­è§£bertæ˜¯æ€éº¼æŠŠä¸åŒèªè¨€éƒ½é—œè¯èµ·ä¾†çš„ã€‚   
å°‡ä¸åŒèªè¨€çš„masklmæ¨¡å‹ï¼Œç”¨é€™å€‹æ–¹å¼æ¥èµ·ä¾†åšç¿»è­¯ä»»å‹™   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img22.png) 
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img23.png)   
   
   
æ¨¡å‹ä¹‹é–“åƒæ•¸å…±äº«æ˜¯æˆåŠŸçš„é—œéµ   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img24.png)   
   
   
é€™æ˜¯å› ç‚ºBertåœ¨èƒŒå¾Œå­¸åˆ°ä¸€å€‹è©èˆ‡ä¸Šä¸‹æ–‡çš„åˆ†ä½ˆã€‚åœ¨ä¸åŒèªè¨€ä¸Šï¼ŒåŒæ¨£æ„æ€çš„è©ï¼Œä¸Šä¸‹æ–‡çš„åˆ†ä½ˆæ‡‰è©²æ˜¯æ¥è¿‘çš„ã€‚   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img25.png)   
   
   
è€ŒBertçš„åƒæ•¸å°±æ˜¯å­¸åˆ°å…¶ä¸­çš„åˆ†ä½ˆï¼Œä½¿å¾—åœ¨å¤šèªè¨€é·ç§»ä¸Šèƒ½æœ‰å¦‚æ­¤é©šè±”çš„æ•ˆæœã€‚   
   
   
## æ›´å¤§çš„æ¨¡å‹ï¼Œæ›´å¥½çš„çµæœ?   
   
å„˜ç®¡Bertå·²ç¶“ç”¨äº†ä¸€å€‹å¾ˆå¤§çš„æ¨¡å‹ï¼Œä½†ç›´è¦ºä¸Šæƒ³ï¼Œè³‡æ–™è¶Šå¤šï¼Œæ¨¡å‹è¶Šå¤§ï¼Œæ•ˆæœæ‡‰è©²è¶Šå¥½ã€‚å› æ­¤ï¼Œå°±æœ‰äº†ä¸€æ³¢è»å‚™ç«¶è³½ï¼š   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img26.png)   
   
   
åœ¨æœ€è¿‘çš„T5ä¸Šæ›´æ˜¯åˆ°é”æ¥µè‡´   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img27.png)   
   
   
ä½†å¾çµæœä¾†çœ‹ï¼Œæ›´å¤§çš„æ¨¡å‹å¥½åƒå¸¶ä¾†çš„å¢å¹…ï¼Œç›¸å°è³‡æ–™ä¾†èªªï¼Œå¤ªå°äº†   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img28.png)   
   
   
å› æ­¤ï¼Œå–®ç´”å¢å¤§æ¨¡å‹ï¼Œä¸¦ä¸æ˜¯æœ€æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç”¨ä¸ä¸€æ¨£çš„è¨“ç·´æ–¹å¼å’Œç›®æ¨™ï¼Œä¹Ÿæ˜¯æå‡çµæœçš„æ–¹æ³•ã€‚   
å¦‚ELECTRAå°±é€šéæ–°çš„è¨“ç·´æ–¹å¼ï¼Œä½¿å¾—æ¯ä¸€å€‹å­—éƒ½æœƒåƒèˆ‡åˆ°æ¨¡å‹ä¸­ï¼Œè®“æ¨¡å‹æ›´åŠ æœ‰æ•ˆå­¸åˆ°representation   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img29.png) 
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img30.png)   
   
   
Albertå‰‡é€šéparameter sharing è®“åƒæ•¸é‡è®Šå°çš„åŒæ™‚ï¼Œæ•ˆæœæ²’æœ‰å¤§å¹…åº¦ä¸‹é™   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img31.png) 
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img32.png)   
   
   
   
## å¤šä»»å‹™   
   
Bertæœ¬èº«å°±æ˜¯ä¸€å€‹å¤šä»»å‹™æ¨¡å‹ï¼Œä¸‹æ¸¸finetuneä¹Ÿå¾ˆè‡ªç„¶æœƒç”¨åˆ°å¤šä»»å‹™   
ä¹‹å‰ä»‹ç´¹é**Multi-Task Deep Neural Networks for Natural Language Understanding(ç°¡ç¨±MTDNN)**æ˜¯æ€éº¼æ¨£åšå¤šä»»å‹™çš„ï¼š   
   
https://voidful.github.io/voidful_blog/paper%20reading/2019/05/06/paper-notes-multi-task-deep-neural-network-for-natural-language-understanding/   
   
   
é‚„ä»‹ç´¹éååˆ†æ¿€é€²çš„GPT2ï¼š   
   
https://voidful.github.io/voidful_blog/paper%20reading/2019/05/13/paper-notes-thinking-openai-gpt2-and-sparse-transformers/   
   
   
æ­¤å¾Œï¼Œåˆæœ‰äº†ä¸€å€‹æ–°çš„å¤šä»»å‹™æ–¹å¼ï¼Œç›¸æ¯”MTDNNæ›´åŠ ç°¡å–®æš´åŠ›ï¼Œç›¸å°GPT2ä¿å®ˆè€Œé‡å¿ƒå‹ƒå‹ƒã€‚é€™å°±æ˜¯Googleçš„T5   
   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img33.png)   
   
   
Googleçš„T5æ¡ç”¨èˆ‡GPT2ååˆ†é¡ä¼¼çš„æƒ³æ³•ï¼Œè¨“ç·´ç”Ÿæˆæ¨¡å‹å»ç”Ÿæˆä¸€åˆ‡æ–‡æœ¬çš„ç­”æ¡ˆï¼Œä½†åœ¨è¨“ç·´æ™‚ï¼Œæœƒè®“æ¨¡å‹çŸ¥é“ç¾åœ¨åœ¨è§£ä¸åŒçš„ä»»å‹™ï¼ŒåŒæ™‚ä¹Ÿè·Ÿbertä¸€æ¨£åˆ†æˆè¨“ç·´å’Œfinetuneå…©å€‹éšæ®µã€‚   
ç”±æ–¼T5éæ–¼è²¡å¤§æ°£ç²—ï¼Œå°è‡´æˆ‘å€‘é€™äº›å¹³æ°‘ä¸å¤ªèƒ½æ‹¿ä¾†ç”¨ã€‚ä½†å¾ä»–å€‘çš„å¯¦é©—çµæœï¼Œå¯èƒ½æœƒæ‰¾åˆ°è§£æ±ºå¤šä»»å‹™å•é¡Œçš„ä¸€äº›å•Ÿç¤ºï¼š   
ç›®å‰å¤šä»»å‹™è¦æ³¨æ„åˆ°è³‡æ–™ä¹‹é–“ä¸å¹³è¡¡çš„å•é¡Œï¼Œä»»å‹™ä¹‹é–“çš„è³‡æ–™é‡æ˜¯ä¸ä¸€æ¨£çš„ï¼Œå°è‡´æ¨¡å‹å°æ–¼æŸäº›è³‡æ–™é‡å°‘çš„ä»»å‹™è¡¨ç¾ä¸ä½³ã€‚   
å…¶ä¸­ä¸€å€‹æ–¹æ³•ï¼Œæ˜¯samplingï¼Œæ¸›å°‘å°è³‡æ–™é‡å¤§çš„è³‡æ–™å–æ¨£ï¼Œå¢åŠ å°å°‘è³‡æ–™çš„å–æ¨£ã€‚å…¶ä¸­ä¸€æ¨£ä¾‹å­ï¼Œä¾¿æ˜¯Bertåœ¨è¨“ç·´å¤šèªè¨€æ™‚ï¼Œä¾¿æ˜¯å¦‚æ­¤å–æ¨£ï¼š 

> To balance these two factors, we performed exponentially smoothed weighting of the data during pre-training data creation (and WordPiece vocab creation). In other words, let's say that the probability of a language is P(L), e.g.,P(English) = 0.21 means that after concatenating all of the Wikipedias together, 21% of our data is English. We exponentiate each probability by some factor S and then re-normalize, and sample from that distribution. In our case we use S=0.7. So, high-resource languages like English will be under-sampled, and low-resource languages like Icelandic will be over-sampled. E.g., in the original distribution English would be sampled 1000x more than Icelandic, but after smoothing it's only sampled 100x more.   

å¦å¤–ä¸€å€‹æ›´åŠ ç°¡å–®çš„æ–¹æ³•ï¼Œå‰‡æ˜¯ä¾†è‡ªT5çš„å¯¦é©—çµæœï¼š   
   
![](https://raw.githubusercontent.com/voidful/voidful_blog/master/assets/post_src/bru2019/img34.png)   
   
   
   
- Unsupervised pre-training + fine-tuningæŒ‡çš„æ˜¯ åšå®ŒT5çš„pre-trainingä¹‹å¾Œï¼Œåœ¨å„å€‹taskä¸Šé¢fine-tuneçš„çµæœ   
- Multi-task training æ˜¯å°‡T5çš„pre-trainingå’Œæ‰€æœ‰taskæ”¾åœ¨ä¸€èµ·è¨“ç·´ï¼Œç›´æ¥åœ¨æ¯ä¸€å€‹taskä¸Šé©—è­‰çµæœ   
- Multi-task pre-training + fine-tuning å‰‡æ˜¯å°‡T5çš„pre-trainingå’Œæ‰€æœ‰taskæ”¾åœ¨ä¸€èµ·è¨“ç·´ï¼Œå†åœ¨æ¯ä¸€å€‹taskçš„è¨“ç·´è³‡æ–™ä¸Šfine-tuneï¼Œç„¶å¾Œé©—è­‰çµæœ   
- Leave-one-out multi-task training æ˜¯å°‡T5çš„pre-trainingå’Œé™¤ç›®æ¨™taskä»¥å¤–çš„taskåšmulti-task trainingï¼Œå†åœ¨ç›®æ¨™taskçš„è³‡æ–™é›†ä¸Šfine-tuneï¼Œç„¶å¾Œé©—è­‰çµæœ   
- Supervised multi-task pre-training å°±ç›´æ¥å°‡æ‰€æœ‰è³‡æ–™åšmulti-task trainingï¼Œç„¶å¾Œåœ¨å„å€‹taskä¸Šé¢fine-tuneçš„çµæœ   
   
ç”±æ­¤å¯è¦‹ï¼Œåœ¨å¤§é‡çš„pertainingè³‡æ–™å¾Œï¼Œåœ¨ç‰¹å®šè³‡æ–™ä¸Šfine-tuneå¯ä»¥ç·©è§£å¤§é‡è³‡æ–™pre-trainingæ™‚å€™ï¼Œè³‡æ–™ä¸å¹³è¡¡çš„å•é¡Œã€‚   
   
   
## ç›¸é—œä¸­æ–‡project   
- [WWMç‰ˆ ä¸­æ–‡ Bert](https://github.com/ymcui/Chinese-BERT-wwm)   
- [ä¸­æ–‡XLNet](https://github.com/brightmart/xlnet_zh)   
- [ä¸­æ–‡Albert](https://github.com/brightmart/albert_zh)      
- [ä¸­æ–‡GPT2](https://github.com/Morizeyao/GPT2-Chinese)      
- [GPT2 for Multiple Languages](https://github.com/imcaspar/gpt2-ml)      

   
## Reference   
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]([https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf))   
[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension]([https://arxiv.org/pdf/1910.13461.pdf](https://arxiv.org/pdf/1910.13461.pdf))   
[SpanBERT: Improving Pre-training by Representing and Predicting Spans]([https://arxiv.org/pdf/1907.10529.pdf](https://arxiv.org/pdf/1907.10529.pdf))   
[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)   
[Cross-lingual Language Model Pretraining](https://arxiv.org/pdf/1901.07291.pdf)   
[Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)   
[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)   
[ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS](https://arxiv.org/pdf/1909.11942.pdf)   
[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)   
[All The Ways You Can Compress BERT](http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html)   
[Bert multilingual](https://github.com/google-research/bert/blob/master/multilingual.md)   
[Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model](https://arxiv.org/pdf/1909.09587.pdf)   
[Bert Multilingual](https://github.com/google-research/bert/blob/master/multilingual.md)   
[ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/forum?id=r1xMH1BtvB)   
[DistilBERT](DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter)   
[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)   
[Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model](https://arxiv.org/pdf/1909.09587.pdf)   
[Emerging Cross-lingual Structure in Pretrained Language Models](https://arxiv.org/pdf/1911.01464.pdf)   
[UNIVERSAL TRANSFORMERS](https://arxiv.org/pdf/1807.03819.pdf)   
   
   